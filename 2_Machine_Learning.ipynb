{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2283h09VDSUiYuf+inW6M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackschreib/J-K-Data-219-FInal-Project/blob/main/2_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j7S8ARlQRqF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jackschreib/Project/main/CSV_Crime_Data_from_2020_to_Present.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df_clean = df[['TIME OCC', 'AREA NAME', 'Premis Desc', 'Crm Cd Desc', 'Vict Age', 'Vict Sex']].dropna()\n",
        "\n",
        "value_counts = df_clean['Crm Cd Desc'].value_counts()\n",
        "rare = value_counts[value_counts < 2].index\n",
        "df_clean = df_clean[~df_clean['Crm Cd Desc'].isin(rare)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 1\n",
        "\n",
        "X_train = df_clean[['TIME OCC', 'Crm Cd Desc', 'Premis Desc', 'Vict Age', 'Vict Sex']]\n",
        "y_train = df_clean['AREA NAME']\n",
        "\n",
        "col_transformer = make_column_transformer(\n",
        "    (StandardScaler(), ['Vict Age']),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), ['Crm Cd Desc', 'Premis Desc', 'Vict Sex']),\n",
        "    remainder='passthrough')\n",
        "\n",
        "distances = ['euclidean','manhattan','minkowski','cosine']\n",
        "for distance in distances:\n",
        "  pipeline = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=20, metric = distance)\n",
        ")\n",
        "\n",
        "  scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "  print(distance, scores.mean())\n",
        "\n",
        "def estimate_test_error(k):\n",
        "  pipeline_knn = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=k, metric = \"cosine\")\n",
        "    )\n",
        "  return cross_val_score(\n",
        "      pipeline_knn,\n",
        "      X=X_train,\n",
        "      y=y_train,\n",
        "      cv=10,\n",
        "      scoring=\"accuracy\"\n",
        "  ).mean()\n",
        "\n",
        "test_errors = pd.Series([])\n",
        "for x in range(1, 21):\n",
        "  error = estimate_test_error(x)\n",
        "  test_errors[x] = error\n",
        "ideal_k = test_errors.idxmin()\n",
        "print(\"The Ideal K is:\", ideal_k)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "  col_transformer,\n",
        "  KNeighborsClassifier(n_neighbors=ideal_k, metric=\"cosine\"))\n",
        "\n",
        "scores = cross_val_score(\n",
        "  pipeline,\n",
        "  X = X_train,\n",
        "  y = y_train,\n",
        "  scoring=\"accuracy\",\n",
        "  cv=10\n",
        ")\n",
        "\n",
        "scores.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOUrJN38RK0H",
        "outputId": "88ba99f0-fd44-4940-829b-25e894f630f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean 0.0813447534141527\n",
            "manhattan 0.08368523484761674\n",
            "minkowski 0.0813447534141527\n",
            "cosine 0.12438946104749107\n",
            "The Ideal K is: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10365577455219052"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using column transformation, we then standardized our numerical features while our categorical features are encoded to analyze both. Next, we assess the performance of a K-Nearest Neighbors (KNN) classifier using various distance metrics including Euclidean, Manhattan, Minkowski, and Cosine to achieve the best distance metric. The printed average accuracy scores for each metric reveal how well the model distinguishes crime locations based on these metrics. Our code then proceeds to identify the optimal number of neighbors (or the K value) for the KNN classifier using cross-validation, finding that K=2 is the ideal value. By evaluating the model's accuracy using the chosen K value and cosine distance metric, the final average accuracy score of approximately 0.1037 is obtained. This score represents the proportion of correct area predictions made by the model during cross-validation, indicating that based on the provided features, this model will not predict the location correctly at a high rate."
      ],
      "metadata": {
        "id": "g04dQnaa7q8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 2\n",
        "\n",
        "X_train = df_clean[['Vict Age', 'Vict Sex']]\n",
        "y_train = df_clean['AREA NAME']\n",
        "\n",
        "col_transformer = make_column_transformer(\n",
        "    (StandardScaler(), ['Vict Age']),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), ['Vict Sex']),\n",
        "    remainder='passthrough')\n",
        "\n",
        "distances = ['euclidean','manhattan','minkowski','cosine']\n",
        "for distance in distances:\n",
        "  pipeline = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=20, metric = distance)\n",
        ")\n",
        "\n",
        "  scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "  print(distance, scores.mean())\n",
        "\n",
        "def estimate_test_error(k):\n",
        "  pipeline_knn = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=k, metric = \"cosine\")\n",
        "    )\n",
        "  return cross_val_score(\n",
        "      pipeline_knn,\n",
        "      X=X_train,\n",
        "      y=y_train,\n",
        "      cv=10,\n",
        "      scoring=\"accuracy\"\n",
        "  ).mean()\n",
        "\n",
        "test_errors = pd.Series([])\n",
        "for x in range(1, 41):\n",
        "  error = estimate_test_error(x)\n",
        "  test_errors[x] = error\n",
        "ideal_k = test_errors.idxmin()\n",
        "print(\"The Ideal K is:\", ideal_k)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "  col_transformer,\n",
        "  KNeighborsClassifier(n_neighbors=ideal_k, metric=\"cosine\"))\n",
        "\n",
        "\n",
        "scores = cross_val_score(\n",
        "  pipeline,\n",
        "  X = X_train,\n",
        "  y = y_train,\n",
        "  scoring=\"accuracy\",\n",
        "  cv=10\n",
        ")\n",
        "\n",
        "scores.mean()\n"
      ],
      "metadata": {
        "id": "MFIOzaS-SnH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ea80f3-4712-4235-a91c-fbd80a60a040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean 0.06974405622027681\n",
            "manhattan 0.06974405622027681\n",
            "minkowski 0.06974405622027681\n",
            "cosine 0.07195732413907104\n",
            "The Ideal K is: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05888105405317158"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We began by standardizing numerical features and encoding categorical features using a column transformer. Again, we evaluated the K-Nearest Neighbors (KNN) classifier's performance utilizing various distance metrics, Euclidean, Manhattan, Minkowski, and Cosine, to determine the best metric. The printed average accuracy scores for each metric provided insight into how effectively the model distinguishes crime locations based on these different distance calculations, showing that Cosine was again the best metric. Moving forward, we utilized cross-validation to find the optimal number of neighbors (K value) for the KNN classifier, discovering that K=1 is the ideal value. However, when we assesed the model's accuracy using the chosen K value and cosine distance metric, we obtained a score of approximately 0.0589. This score unfortunately shows the proportion of correct area predictions made by the model during cross-validation, and that the model is unable to predict the location correctly at a high rate."
      ],
      "metadata": {
        "id": "B8ceFbvfLx-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 3\n",
        "\n",
        "X_train = df_clean[['Crm Cd Desc', 'Premis Desc']]\n",
        "y_train = df_clean['AREA NAME']\n",
        "\n",
        "col_transformer = make_column_transformer(\n",
        "    (OneHotEncoder(handle_unknown='ignore'), ['Crm Cd Desc', 'Premis Desc']),\n",
        "    remainder='passthrough')\n",
        "\n",
        "distances = ['euclidean','manhattan','minkowski','cosine']\n",
        "for distance in distances:\n",
        "  pipeline = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=20, metric = distance)\n",
        ")\n",
        "\n",
        "  scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10,\n",
        "    scoring=\"accuracy\"\n",
        ")\n",
        "\n",
        "  print(distance, scores.mean())\n",
        "\n",
        "def estimate_test_error(k):\n",
        "  pipeline_knn = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsClassifier(n_neighbors=k, metric = \"cosine\")\n",
        "    )\n",
        "  return cross_val_score(\n",
        "      pipeline_knn,\n",
        "      X=X_train,\n",
        "      y=y_train,\n",
        "      cv=10,\n",
        "      scoring=\"accuracy\"\n",
        "  ).mean()\n",
        "\n",
        "test_errors = pd.Series([])\n",
        "for x in range(1, 41):\n",
        "  error = estimate_test_error(x)\n",
        "  test_errors[x] = error\n",
        "ideal_k = test_errors.idxmin()\n",
        "print(\"The Ideal K is:\", ideal_k)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "  col_transformer,\n",
        "  KNeighborsClassifier(n_neighbors=10, metric=\"cosine\"))\n",
        "\n",
        "\n",
        "scores = cross_val_score(\n",
        "  pipeline,\n",
        "  X = X_train,\n",
        "  y = y_train,\n",
        "  scoring=\"accuracy\",\n",
        "  cv=10\n",
        ")\n",
        "\n",
        "scores.mean()"
      ],
      "metadata": {
        "id": "dAlWxiprUetC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c3dd07-7d5f-49fe-c5b7-4e31ee3cd199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "euclidean 0.10930323134494921\n",
            "manhattan 0.10930323134494921\n",
            "minkowski 0.10930323134494921\n",
            "cosine 0.11194919543139378\n",
            "The Ideal K is: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10336317958515469"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We started again by standardizing our numerical features and encoding categorical features through column transformation. After, we evaluated the performance of the K-Nearest Neighbors (KNN) classifier using various distance metrics, including Euclidean, Manhattan, Minkowski, and Cosine, aiming to identify the most appropriate metric. The recorded average accuracy scores for each metric showed the model's capability to differentiate crime locations based on the selected distance calculations, showing again Cosine as the best possible option. Moving forward, we employed cross-validation to determine the optimal number of neighbors (K value) for the KNN classifier, revealing that K=1 is the ideal K value. However, upon assessing the model's accuracy using the chosen K value and cosine distance metric, we got a final average accuracy score of approximately 0.1034. This score shows the proportion of correct area predictions made by the model during cross-validation, indicating that based on the provided features the model struggles again to predict the location correctly at a high rate, showing how difficult it is to get this from a set of features."
      ],
      "metadata": {
        "id": "QAU7s6XNm4V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model 4\n",
        "\n",
        "X_train = df_clean[['AREA NAME', 'Crm Cd Desc', 'Premis Desc', 'Vict Age', 'Vict Sex']]\n",
        "y_train = df_clean['TIME OCC']\n",
        "\n",
        "col_transformer = make_column_transformer(\n",
        "    (StandardScaler(), ['Vict Age']),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), ['Crm Cd Desc', 'Premis Desc', 'Vict Sex','AREA NAME']),\n",
        "    remainder='passthrough')\n",
        "\n",
        "distances = ['euclidean','manhattan','minkowski','cosine']\n",
        "for distance in distances:\n",
        "  pipeline = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsRegressor(n_neighbors=20, metric = distance)\n",
        ")\n",
        "\n",
        "  scores = cross_val_score(\n",
        "    pipeline,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10,\n",
        "    scoring=\"neg_mean_squared_error\"\n",
        ")\n",
        "\n",
        "  print(distance, np.sqrt(-scores.mean()))\n",
        "\n",
        "def estimate_test_error(k):\n",
        "  pipeline_knn = make_pipeline(\n",
        "    col_transformer,\n",
        "    KNeighborsRegressor(n_neighbors=k, metric = \"manhattan\")\n",
        "    )\n",
        "  return np.sqrt(-cross_val_score(\n",
        "      pipeline_knn,\n",
        "      X=X_train,\n",
        "      y=y_train,\n",
        "      cv=10,\n",
        "      scoring=\"neg_mean_squared_error\"\n",
        "  ).mean())\n",
        "\n",
        "test_errors = pd.Series([])\n",
        "for x in range(1, 41):\n",
        "  error = estimate_test_error(x)\n",
        "  test_errors[x] = error\n",
        "ideal_k = test_errors.idxmin()\n",
        "print(\"The Ideal K is:\", ideal_k)\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "  col_transformer,\n",
        "  KNeighborsRegressor(n_neighbors=ideal_k, metric=\"manhattan\"))\n",
        "\n",
        "\n",
        "scores = -cross_val_score(\n",
        "    pipeline,\n",
        "    X = X_train,\n",
        "    y = y_train,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "scores.mean()"
      ],
      "metadata": {
        "id": "A4TR9IoyU3dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fcc151-681b-4fc1-866e-8f86201b31f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "euclidean 657.00833323198\n",
            "manhattan 656.8464646804752\n",
            "minkowski 657.00833323198\n",
            "cosine 656.9618766895911\n",
            "The Ideal K is: 40\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "422420.22666616226"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to complications with our models above, we decided to switch to attempting to predict the time of the crime rather than the area itself. This code builds a predictive model for estimating the time of occurrence of crimes based on various features such as the area name, crime code description, premises description, victim age, and victim sex. It sets up a pipeline for preprocessing the features using standard scaling for numerical features (victim age) and one-hot encoding for categorical features (crime code description, premises description, victim sex, and area name). Next, it runs through different distance metrics (euclidean, manhattan, minkowski, and cosine) for the K-nearest neighbors (KNN) regressor within a cross-validation loop to find the best-performing distance metric. The negative mean squared error is computed for each distance metric, and the square root of the average error is printed, resulting in manhattan being the best. It then identifies the ideal value of K (number of neighbors) for the KNN regressor by finding the K value that minimizes the cross-validated mean squared error. Once the ideal K, which is 40, is determined, the code builds a final pipeline using the optimal K value and Manhattan distance metric. Finally, it evaluates the performance of the model by computing the mean squared error using cross-validation with the optimal pipeline. The resulting mean squared error represents the average squared difference between the predicted and actual time of crime occurrences across the cross-validation folds. The mean squared error obtained from the cross-validation provides the reported mean squared error of approximately 422,420.23, indicating the average squared difference between the predicted and actual time of crime occurrences, suggesting that the model has a reasonable predictive capability of a correct prediction."
      ],
      "metadata": {
        "id": "di0BC9thniy4"
      }
    }
  ]
}